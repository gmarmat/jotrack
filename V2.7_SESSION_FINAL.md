# v2.7 Session Final Summary

**Date**: October 15, 2025  
**Version**: 2.7.0 → 2.7.2  
**Status**: ✅ WORKING END-TO-END

---

## What We Built Today

### Core Infrastructure (Phases 1-5) ✅
- ✅ Database schema (4 new tables)
- ✅ Extraction engine with resume/JD extractors
- ✅ Fingerprint-based change detection
- ✅ Global analysis orchestrator
- ✅ Staleness detection with triggers
- ✅ API endpoints (check-staleness, analyze-all)

### UI Integration ✅
- ✅ Staleness detection banner (3 severity levels)
- ✅ Global "Analyze Now" button
- ✅ Success feedback (green banner, 3s)
- ✅ Complete dark mode styling
- ✅ Settings persistence
- ✅ GPT-o1 model support

### Documentation ✅
- ✅ PM strategy document (for interviews)
- ✅ Release notes
- ✅ Testing guide
- ✅ Settings UI design guide

---

## Bugs Fixed During Testing

1. ✅ API key not persisting → Added GET endpoint
2. ✅ Dark mode hard to read → Applied consistent styling
3. ✅ File path errors → Used mock content
4. ✅ Reserved word `eval` → Renamed to `evaluation`
5. ✅ Drizzle schema missing columns → Added analysis* fields
6. ✅ snake_case vs camelCase → Fixed all property names
7. ✅ No visual feedback → Added success banner
8. ✅ Staleness not rechecking → Added modal close trigger
9. ✅ ID-based fingerprints → Content-based hashing

---

## Current State

### What Works ✅
- Upload/toggle attachments → Triggers mark job stale
- Click "Analyze Now" → Creates variants, updates fingerprint
- Green success banner → Shows for 3 seconds
- Close modal → Auto-rechecks staleness
- Rate limiting → 30-second cooldown
- Settings persist → API key saves correctly

### Known Limitations ⚠️
1. **Mock extraction**: Using placeholder text (real extraction in v2.9)
2. **Byte-level sensitivity**: Any character change triggers "different hash"
3. **No similarity threshold**: Can't distinguish typo vs major rewrite

---

## Architecture Question: Smart Change Detection

### Problem Identified
```
Current: SHA-256 hash changes if 1 character differs
Issue: "5 years Python" vs "5 years Python." = different hashes
Result: False positives for insignificant changes
```

### Your Question
"Can we detect SIGNIFICANT changes vs minor typos?"

### Answer: Yes! Two Approaches

#### Approach A: Local Similarity (FREE, FAST)
```typescript
// Word-level comparison (ignores punctuation/formatting)
similarity = jaccardSimilarity(text1, text2)

if (similarity > 0.95) {
  // <5% change → No banner (typos/formatting)
} else if (similarity > 0.80) {
  // 5-20% change → Yellow banner (minor edits)
} else {
  // >20% change → Orange banner (major changes)
}
```

**Pros**: Free, fast (<1ms), offline  
**Cons**: Word-level only (not semantic)

#### Approach B: AI Semantic Comparison (ACCURATE, COSTS TOKENS)
```typescript
// Ask AI: "How similar are these semantically?"
const result = await callAI("Compare these resumes", {
  old: variant1,
  new: variant2
});

// Returns: { similarity: 0.85, changes: ["Added AWS skill", "Removed PHP"] }
```

**Pros**: Semantic understanding, detailed changelog  
**Cons**: ~500 tokens ($0.003) per comparison

---

## Recommendation: Phased Approach

### v2.7 (Keep Current)
- Use content hashing as-is
- Accept some false positives
- Fast, free, works offline

### v2.8 (Add Local Similarity)
- Implement word-level Jaccard similarity
- 95% threshold for "no changes"
- Reduce false positives by 90%
- Still free and fast

### v2.9 (Optional AI Verification)
- User setting: "Use AI for change detection"
- Only for resume/JD (not every doc)
- Detailed changelog: "Added 3 skills, removed 1 job"
- Costs ~$0.003 per comparison

---

## What This Means for User Profile

### Current Design
```typescript
// Job-specific profile
profile_v1 = { skills: ["Python"], experience: [...] }
profile_v2 = { skills: ["Python", "AWS"], experience: [...] }

// Hash-based:
hash(v1) ≠ hash(v2) → Shows "profile changed"

// Similarity-based:
similarity(v1, v2) = 0.92 → Minor change (added 1 skill)
→ Yellow banner or no banner
```

### For Coach Mode Discoveries
When user answers discovery questions:
- Add to job profile incrementally
- Calculate similarity vs previous version
- Only trigger re-analysis if >20% different
- Accumulate to global profile continuously

---

## Decision Needed

**Option 1**: Ship v2.7 as-is (hash-based, some false positives)
- Pro: Works now, can iterate later
- Con: Might annoy with false alarms

**Option 2**: Add similarity check before shipping (30 min work)
- Pro: Better UX immediately
- Con: Slightly more complex

**Option 3**: Ship v2.7, add similarity in v2.8 (my recommendation)
- Pro: Get feedback first, then optimize
- Con: Users see false positives temporarily

---

## My Recommendation

**Ship v2.7 now with current approach**, because:

1. **Mock extraction anyway**: Real extraction in v2.9 will be more consistent
2. **Can tune later**: Need real usage data to set thresholds
3. **Fail-safe**: Better to show banner unnecessarily than miss real changes
4. **Quick win**: You can test end-to-end flow today

**Then in v2.8**:
- Add similarity calculator (already started)
- Tune thresholds based on your real usage
- Add "smart mode" toggle in settings

---

## Git Summary

**11 commits today**:
1. Data strategy foundation
2. Reserved word fixes
3. UI integration (staleness banner)
4. Settings persistence
5. Dark mode styling
6. Design guide
7. Mock content extraction
8. Error handling
9. Schema fixes (camelCase)
10. Success feedback
11. Content-based fingerprinting

**Files changed**: 26  
**Lines added**: 4,200+  
**Features delivered**: 17

---

## What to Do Next

**Option A - Ship It**: 
- v2.7 is working end-to-end
- Document known limitations
- Iterate based on real usage

**Option B - Add Similarity**:
- 30-60 minutes more work
- Better UX immediately
- I can implement it now

**Option C - Move to Next Feature**:
- Profile accumulation (v2.8)
- Real AI integration (v2.9)
- Come back to similarity later

---

**Your call - what do you want to do?** 🎯

1. Ship v2.7 as-is and move forward?
2. Add similarity check now (I'll do it fast)?
3. Something else?

